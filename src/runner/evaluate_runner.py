"""Evaluation Runner"""
import sys
import json
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any, Tuple
from tqdm import tqdm

import torch
import pandas as pd
import pytorch_lightning as pl
from torch.utils.data import DataLoader
from torchmetrics.detection.mean_ap import MeanAveragePrecision
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    confusion_matrix, classification_report
)

from ..registry import MODEL_REGISTRY, DATASET_REGISTRY
from ..utils.visualization import (
    save_detection_images, 
    plot_confusion_matrix, 
    plot_class_metrics,
    create_evaluation_report
)

def box_iou(boxes1, boxes2):
    """
    CXCYWH í˜•ì‹ì˜ ë°•ìŠ¤ IoU ê³„ì‚°
    """
    from torchvision.ops import box_iou as tv_box_iou
    
    # CXCYWH -> XYXY ë³€í™˜
    def cxcywh_to_xyxy(boxes):
        cx, cy, w, h = boxes.unbind(-1)
        x1 = cx - w / 2
        y1 = cy - h / 2
        x2 = cx + w / 2
        y2 = cy + h / 2
        return torch.stack([x1, y1, x2, y2], dim=-1)
    
    boxes1_xyxy = cxcywh_to_xyxy(boxes1)
    boxes2_xyxy = cxcywh_to_xyxy(boxes2)
    
    return tv_box_iou(boxes1_xyxy, boxes2_xyxy)


class EvaluationRunner:
    """í‰ê°€ ì‹¤í–‰ í´ë˜ìŠ¤ - ìƒì„¸ ë¶„ì„ í¬í•¨"""
    
    def __init__(self, config, checkpoint_path, split='test', output_dir=None):
        self.config = config
        self.checkpoint_path = checkpoint_path
        self.split = split
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ì„¤ì •
        if output_dir is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            checkpoint_name = Path(checkpoint_path).stem
            output_dir = f"results/evaluation_{checkpoint_name}_{split}_{timestamp}"
        
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # í‰ê°€ ì„¤ì •
        self.score_threshold = config.get('evaluation', {}).get('score_threshold', 0.5)
        self.iou_threshold = config.get('evaluation', {}).get('iou_threshold', 0.5)
        
        print(f"Evaluation output directory: {self.output_dir}")
        print(f"Score threshold: {self.score_threshold}")
        print(f"IoU threshold: {self.iou_threshold}")
    
    def _load_model_and_data(self):
        """ëª¨ë¸ê³¼ ë°ì´í„° ë¡œë“œ"""
        model_name = self.config['model']['arch_name']
        dataset_name = self.config['data']['dataset_name']
        
        # Dataset ë©”íƒ€ì •ë³´
        dataset_meta = DATASET_REGISTRY.get(dataset_name)
        
        # ëª¨ë¸ë³„ ì²˜ë¦¬
        if model_name == "DETR" or model_name == "detr":
            from transformers import DetrImageProcessor
            from ..data.transforms.detr_transform import create_detr_dataset, DetrCocoDataset
            
            imageprocessor = DetrImageProcessor.from_pretrained(
                self.config['model']['pretrained_path']
            )
            
            # í‰ê°€í•  ë°ì´í„°ì…‹ ë¡œë“œ
            dataset = create_detr_dataset(dataset_meta, self.split, imageprocessor, self.config)
            collate_fn = DetrCocoDataset.create_collate_fn(imageprocessor)
            
            # ì²´í¬í¬ì¸íŠ¸ì—ì„œ ëª¨ë¸ ë¡œë“œ
            ModelClass = MODEL_REGISTRY[model_name]
            model = ModelClass.load_from_checkpoint(
                self.checkpoint_path,
                num_labels=self.config['model']['num_labels'],
                pretrained_path=self.config['model']['pretrained_path'],
                lr=self.config['model']['learning_rate'],
                lr_backbone=self.config['model']['lr_backbone'],
                weight_decay=self.config['model']['weight_decay'],
            )
            
        elif model_name in ["YOLOv11", "YOLOv12"]:
            from ..data.transforms.yolo_transform import create_yolo_dataset
            
            dataset = create_yolo_dataset(
                dataset_meta, self.split, self.config['data']['image_size']
            )
            collate_fn = None
            
            ModelClass = MODEL_REGISTRY[model_name]
            model = ModelClass.load_from_checkpoint(
                self.checkpoint_path,
                model_size=self.config['model']['model_size'],
                num_classes=self.config['model']['num_labels'],
                lr=self.config['model']['learning_rate'],
            )
        
        else:
            raise ValueError(f"Unknown model: {model_name}")
        
        model.eval()
        return model, dataset, collate_fn
    
    def _create_dataloader(self, dataset, collate_fn):
        """DataLoader ìƒì„±"""
        batch_size = self.config.get('evaluation', {}).get('batch_size', 
                                                          self.config['data']['batch_size'])
        
        return DataLoader(
            dataset,
            batch_size=batch_size,
            shuffle=False,
            num_workers=self.config['data'].get('num_workers', 0),
            collate_fn=collate_fn
        )
    
    def _evaluate_detection_metrics_detailed(self, model, dataloader) -> Tuple[Dict, Dict, List, List]:
        """
        ìƒì„¸í•œ Detection ë©”íŠ¸ë¦­ ê³„ì‚° (TP/FP/FN ë¶„ì„ í¬í•¨)
        
        Returns:
            detection_metrics: mAP ë“± detection ë©”íŠ¸ë¦­
            detailed_stats: TP/FP/FN ìƒì„¸ í†µê³„
            all_predictions: ëª¨ë“  ì˜ˆì¸¡ ê²°ê³¼
            all_targets: ëª¨ë“  íƒ€ê²Ÿ ê²°ê³¼
        """
        device = next(model.parameters()).device
    
        # Detection metric ì´ˆê¸°í™”
        map_metric = MeanAveragePrecision(
            box_format="cxcywh", 
            iou_type="bbox",
            class_metrics=True
        )
        
        # ìƒì„¸ í†µê³„ë¥¼ ìœ„í•œ ë³€ìˆ˜ë“¤
        total_predictions = 0
        total_ground_truths = 0
        total_tp = 0
        total_fp = 0
        total_fn = 0
        
        # í´ë˜ìŠ¤ë³„ í†µê³„
        num_classes = self.config['model']['num_labels']
        class_tp = [0] * num_classes
        class_fp = [0] * num_classes
        class_fn = [0] * num_classes
        class_predictions = [0] * num_classes
        class_ground_truths = [0] * num_classes
        
        all_predictions = []
        all_targets = []
        
        print(f"\n{self.split.upper()} ë°ì´í„° í‰ê°€ ì¤‘...")
    
        with torch.no_grad():
            for batch_idx, batch in enumerate(tqdm(dataloader, desc="Evaluating")):
                if self.config.get('debug') and batch_idx >= 5:
                    break
                
                pixel_values = batch["pixel_values"].to(device)
                pixel_mask = batch.get("pixel_mask")
                if pixel_mask is not None:
                    pixel_mask = pixel_mask.to(device)
                
                labels = [{k: v.to(device) for k, v in t.items()} for t in batch["labels"]]
                
                # ëª¨ë¸ ì¶”ë¡ 
                outputs = model(pixel_values, pixel_mask)
                
                # Detection metricìš© ë°ì´í„° ì¤€ë¹„
                probs = outputs.logits.softmax(-1)[..., :-1]
                scores, pred_labels = probs.max(-1)
                pred_boxes = outputs.pred_boxes
                
                preds = []
                targets = []
                
                for i in range(pred_boxes.shape[0]):
                    # Prediction í•„í„°ë§
                    keep = scores[i] > self.score_threshold
                    
                    pred_boxes_filtered = pred_boxes[i][keep].detach().cpu()
                    pred_scores_filtered = scores[i][keep].detach().cpu()
                    pred_labels_filtered = pred_labels[i][keep].detach().cpu()
                    
                    preds.append({
                        "boxes": pred_boxes_filtered,
                        "scores": pred_scores_filtered,
                        "labels": pred_labels_filtered,
                    })
                    
                    # Ground truth
                    gt_boxes = labels[i]["boxes"].detach().cpu()
                    gt_labels = labels[i]["class_labels"].detach().cpu()
                    
                    targets.append({
                        "boxes": gt_boxes,
                        "labels": gt_labels,
                    })
                    
                    # ìƒì„¸ TP/FP/FN ê³„ì‚°
                    num_preds = len(pred_boxes_filtered)
                    num_gts = len(gt_boxes)
                    
                    # í´ë˜ìŠ¤ë³„ GT ì¹´ìš´íŠ¸
                    for gt_label in gt_labels:
                        class_ground_truths[gt_label.item()] += 1
                    
                    # í´ë˜ìŠ¤ë³„ ì˜ˆì¸¡ ì¹´ìš´íŠ¸
                    for pred_label in pred_labels_filtered:
                        class_predictions[pred_label.item()] += 1
                    
                    if num_preds > 0 and num_gts > 0:
                        # IoU ê³„ì‚°
                        iou_matrix = box_iou(pred_boxes_filtered, gt_boxes)
                        
                        # ê° ì˜ˆì¸¡ì— ëŒ€í•´ ê°€ì¥ ë†’ì€ IoUë¥¼ ê°€ì§„ GT ì°¾ê¸°
                        max_ious, matched_gt_indices = iou_matrix.max(dim=1)
                        
                        # TP: IoU >= thresholdì¸ ì˜ˆì¸¡
                        tp_mask = max_ious >= self.iou_threshold
                        tp = tp_mask.sum().item()
                        
                        # FP: IoU < thresholdì¸ ì˜ˆì¸¡
                        fp = (~tp_mask).sum().item()
                        
                        # FN: ë§¤ì¹­ë˜ì§€ ì•Šì€ GT
                        matched_gt_mask = torch.zeros(num_gts, dtype=torch.bool)
                        if tp > 0:
                            matched_gt_mask[matched_gt_indices[tp_mask]] = True
                        fn = (~matched_gt_mask).sum().item()
                        
                        # í´ë˜ìŠ¤ë³„ TP/FP/FN ê³„ì‚°
                        for j, (pred_label, is_tp) in enumerate(zip(pred_labels_filtered, tp_mask)):
                            if is_tp:
                                class_tp[pred_label.item()] += 1
                            else:
                                class_fp[pred_label.item()] += 1
                        
                        # ë§¤ì¹­ë˜ì§€ ì•Šì€ GTëŠ” í•´ë‹¹ í´ë˜ìŠ¤ì˜ FN
                        for j, gt_label in enumerate(gt_labels):
                            if not matched_gt_mask[j]:
                                class_fn[gt_label.item()] += 1
                        
                    elif num_preds > 0 and num_gts == 0:
                        # GTê°€ ì—†ëŠ”ë° ì˜ˆì¸¡ì´ ìˆìŒ -> ëª¨ë‘ FP
                        tp = 0
                        fp = num_preds
                        fn = 0
                        
                        for pred_label in pred_labels_filtered:
                            class_fp[pred_label.item()] += 1
                        
                    elif num_preds == 0 and num_gts > 0:
                        # ì˜ˆì¸¡ì´ ì—†ëŠ”ë° GTê°€ ìˆìŒ -> ëª¨ë‘ FN
                        tp = 0
                        fp = 0
                        fn = num_gts
                        
                        for gt_label in gt_labels:
                            class_fn[gt_label.item()] += 1
                        
                    else:
                        tp = fp = fn = 0
                    
                    total_tp += tp
                    total_fp += fp
                    total_fn += fn
                    total_predictions += num_preds
                    total_ground_truths += num_gts
                
                all_predictions.extend(preds)
                all_targets.extend(targets)
                
                # Detection metric ì—…ë°ì´íŠ¸
                if preds:
                    map_metric.update(preds, targets)
    
        # Detection ì§€í‘œ ê³„ì‚°
        detection_metrics = map_metric.compute()
        
        # ì „ì²´ Precision, Recall, F1 ê³„ì‚°
        overall_precision = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0
        overall_recall = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0
        overall_f1 = 2 * (overall_precision * overall_recall) / (overall_precision + overall_recall) if (overall_precision + overall_recall) > 0 else 0
        
        # í´ë˜ìŠ¤ë³„ Precision, Recall, F1 ê³„ì‚°
        class_precision = []
        class_recall = []
        class_f1 = []
        
        for i in range(num_classes):
            prec = class_tp[i] / (class_tp[i] + class_fp[i]) if (class_tp[i] + class_fp[i]) > 0 else 0
            rec = class_tp[i] / (class_tp[i] + class_fn[i]) if (class_tp[i] + class_fn[i]) > 0 else 0
            f1 = 2 * (prec * rec) / (prec + rec) if (prec + rec) > 0 else 0
            
            class_precision.append(prec)
            class_recall.append(rec)
            class_f1.append(f1)
        
        # ìƒì„¸ í†µê³„ ë”•ì…”ë„ˆë¦¬
        detailed_stats = {
            "total_statistics": {
                "total_ground_truths": total_ground_truths,
                "total_predictions": total_predictions,
                "total_tp": total_tp,
                "total_fp": total_fp,
                "total_fn": total_fn,
                "overall_precision": overall_precision,
                "overall_recall": overall_recall,
                "overall_f1": overall_f1
            },
            "class_statistics": {
                "class_tp": class_tp,
                "class_fp": class_fp,
                "class_fn": class_fn,
                "class_predictions": class_predictions,
                "class_ground_truths": class_ground_truths,
                "class_precision": class_precision,
                "class_recall": class_recall,
                "class_f1": class_f1
            }
        }
        
        return detection_metrics, detailed_stats, all_predictions, all_targets
    
    def _evaluate_classification_metrics(self, all_predictions, all_targets):
        """Classification ë©”íŠ¸ë¦­ ê³„ì‚°"""
        all_true_labels = []
        all_pred_labels = []
        
        # Detection ê²°ê³¼ë¥¼ Classificationìœ¼ë¡œ ë³€í™˜
        for pred, target in zip(all_predictions, all_targets):
            gt_labels = target["labels"]
            pred_labels = pred["labels"]
        
            if len(gt_labels) > 0:
                for gt_label in gt_labels:
                    all_true_labels.append(gt_label.item())
                
                if len(pred_labels) > 0:
                    # ê°„ë‹¨í•œ ë§¤ì¹­: ì˜ˆì¸¡ ìˆ˜ë§Œí¼ GTì—ì„œ ê°€ì ¸ì˜¤ê¸°
                    for i, pred_label in enumerate(pred_labels):
                        if i < len(gt_labels):
                            all_pred_labels.append(pred_label.item())
                        else:
                            # ì˜ˆì¸¡ì´ GTë³´ë‹¤ ë§ìœ¼ë©´ ë§ˆì§€ë§‰ GT ë¼ë²¨ ì‚¬ìš©
                            all_pred_labels.append(gt_labels[-1].item())
                else:
                    # ì˜ˆì¸¡ì´ ì—†ìœ¼ë©´ "no detection" í´ë˜ìŠ¤ë¡œ ì²˜ë¦¬ (ì—¬ê¸°ì„œëŠ” -1)
                    for _ in gt_labels:
                        all_pred_labels.append(-1)
    
        if len(all_true_labels) == 0 or len(all_pred_labels) == 0:
            return {}
        
        # ê¸¸ì´ ë§ì¶”ê¸°
        min_len = min(len(all_true_labels), len(all_pred_labels))
        all_true_labels = all_true_labels[:min_len]
        all_pred_labels = all_pred_labels[:min_len]
        
        # -1 (no detection) ì œê±°
        valid_indices = [i for i, pred in enumerate(all_pred_labels) if pred >= 0]
        all_true_labels = [all_true_labels[i] for i in valid_indices]
        all_pred_labels = [all_pred_labels[i] for i in valid_indices]
        
        if len(all_true_labels) == 0:
            return {}
        
        # ë©”íŠ¸ë¦­ ê³„ì‚°
        num_classes = self.config['model']['num_labels']
        class_names = self.config['data']['class_names']
        
        results = {
            "accuracy": accuracy_score(all_true_labels, all_pred_labels),
            "precision_macro": precision_score(all_true_labels, all_pred_labels, average='macro', zero_division=0),
            "recall_macro": recall_score(all_true_labels, all_pred_labels, average='macro', zero_division=0),
            "f1_macro": f1_score(all_true_labels, all_pred_labels, average='macro', zero_division=0),
            "confusion_matrix": confusion_matrix(all_true_labels, all_pred_labels, labels=list(range(num_classes))).tolist(),
            "classification_report": classification_report(
                all_true_labels, all_pred_labels,
                target_names=class_names,
                labels=list(range(num_classes)),
                output_dict=True,
                zero_division=0
            )
        }
        
        return results
    
    def _save_results_with_visualization(self, detection_metrics, detailed_stats, classification_results):
        """ê²°ê³¼ ì €ì¥ ë° ì‹œê°í™”"""
        
        # í…ì„œë¥¼ ì ì ˆí•œ í˜•íƒœë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜
        def convert_tensor_to_serializable(obj):
            if torch.is_tensor(obj):
                if obj.numel() == 1:  # ìŠ¤ì¹¼ë¼ í…ì„œ
                    return obj.item()
                else:  # ë°°ì—´ í…ì„œ
                    return obj.tolist()
            elif isinstance(obj, np.ndarray):
                return obj.tolist()
            elif isinstance(obj, dict):
                return {k: convert_tensor_to_serializable(v) for k, v in obj.items()}
            elif isinstance(obj, list):
                return [convert_tensor_to_serializable(item) for item in obj]
            else:
                return obj
        
        # 1. JSON ê²°ê³¼ ì €ì¥
        results = {
            "evaluation_info": {
                "checkpoint": str(self.checkpoint_path),
                "split": self.split,
                "timestamp": datetime.now().isoformat(),
                "score_threshold": self.score_threshold,
                "iou_threshold": self.iou_threshold,
                "config": self.config
            },
            "detection_metrics": convert_tensor_to_serializable(detection_metrics),
            "detailed_statistics": detailed_stats,
            "classification_metrics": classification_results
        }
        
        results_file = self.output_dir / "evaluation_results.json"
        with open(results_file, 'w') as f:
            json.dump(results, f, indent=2, default=str)
        
            # 2. ì „ì²´ ì„±ëŠ¥ ìš”ì•½ í‘œ ìƒì„±
        summary_data = {
            'Metric': [
                'mAP@0.5', 'mAP@0.5:0.95', 'mAP@0.75',
                'Overall Precision', 'Overall Recall', 'Overall F1-Score',
                'Total Ground Truths', 'Total Predictions', 'True Positives', 'False Positives', 'False Negatives'
            ],
            'Value': [
                f"{detection_metrics['map_50']:.4f}",
                f"{detection_metrics['map']:.4f}",
                f"{detection_metrics['map_75']:.4f}",
                f"{detailed_stats['total_statistics']['overall_precision']:.4f}",
                f"{detailed_stats['total_statistics']['overall_recall']:.4f}",
                f"{detailed_stats['total_statistics']['overall_f1']:.4f}",
                detailed_stats['total_statistics']['total_ground_truths'],
                detailed_stats['total_statistics']['total_predictions'],
                detailed_stats['total_statistics']['total_tp'],
                detailed_stats['total_statistics']['total_fp'],
                detailed_stats['total_statistics']['total_fn']
            ]
        }
        
        summary_df = pd.DataFrame(summary_data)
        summary_csv = self.output_dir / "summary_metrics.csv"
        summary_df.to_csv(summary_csv, index=False)
        
        # 3. í´ë˜ìŠ¤ë³„ ìƒì„¸ ì„±ëŠ¥ í‘œ ìƒì„±
        class_names = self.config['data']['class_names']
        class_stats = detailed_stats['class_statistics']
        
        class_data = []
        for i, class_name in enumerate(class_names):
            if i < len(class_stats['class_tp']):
                # Detection ë©”íŠ¸ë¦­ì—ì„œ í´ë˜ìŠ¤ë³„ mAP ì¶”ì¶œ
                class_map = "N/A"
                if 'map_per_class' in detection_metrics:
                    map_per_class = convert_tensor_to_serializable(detection_metrics['map_per_class'])
                    if isinstance(map_per_class, list) and i < len(map_per_class):
                        class_map = f"{map_per_class[i]:.4f}"
                    elif not isinstance(map_per_class, list):
                        class_map = f"{map_per_class:.4f}" if i == 0 else "N/A"
                
                class_data.append({
                    'Class': class_name,
                    'Ground Truth Count': class_stats['class_ground_truths'][i],
                    'Prediction Count': class_stats['class_predictions'][i],
                    'True Positives': class_stats['class_tp'][i],
                    'False Positives': class_stats['class_fp'][i],
                    'False Negatives': class_stats['class_fn'][i],
                    'Precision': f"{class_stats['class_precision'][i]:.4f}",
                    'Recall': f"{class_stats['class_recall'][i]:.4f}",
                    'F1-Score': f"{class_stats['class_f1'][i]:.4f}",
                    'mAP@0.5:0.95': class_map
                })
        
        class_df = pd.DataFrame(class_data)
        class_csv = self.output_dir / "class_metrics.csv"
        class_df.to_csv(class_csv, index=False)
        
        # 4. Classification ê²°ê³¼ í‘œ (ìˆëŠ” ê²½ìš°)
        if classification_results and 'classification_report' in classification_results:
            clf_report = classification_results['classification_report']
            clf_data = []
            
            for class_name in class_names:
                if class_name in clf_report:
                    clf_data.append({
                        'Class': class_name,
                        'Precision': f"{clf_report[class_name]['precision']:.4f}",
                        'Recall': f"{clf_report[class_name]['recall']:.4f}",
                        'F1-Score': f"{clf_report[class_name]['f1-score']:.4f}",
                        'Support': clf_report[class_name]['support']
                    })
            
            # ì „ì²´ í‰ê·  ì¶”ê°€
            if 'macro avg' in clf_report:
                clf_data.append({
                    'Class': 'Macro Average',
                    'Precision': f"{clf_report['macro avg']['precision']:.4f}",
                    'Recall': f"{clf_report['macro avg']['recall']:.4f}",
                    'F1-Score': f"{clf_report['macro avg']['f1-score']:.4f}",
                    'Support': clf_report['macro avg']['support']
                })
            
            clf_df = pd.DataFrame(clf_data)
            clf_csv = self.output_dir / "classification_metrics.csv"
            clf_df.to_csv(clf_csv, index=False)
        
        # 5. ì‹œê°í™” (Confusion Matrix)
        if classification_results and 'confusion_matrix' in classification_results:
            cm = np.array(classification_results['confusion_matrix'])
            class_names = self.config['data']['class_names']
            
            plt.figure(figsize=(10, 8))
            sns.heatmap(
                cm, 
                annot=True, 
                fmt='d', 
                cmap='Blues',
                xticklabels=class_names,
                yticklabels=class_names
            )
            plt.title(f'Confusion Matrix - {self.split.upper()} Set')
            plt.ylabel('True Label')
            plt.xlabel('Predicted Label')
            plt.tight_layout()
            
            cm_file = self.output_dir / "confusion_matrix.png"
            plt.savefig(cm_file, dpi=300, bbox_inches='tight')
            plt.close()
            
            print(f"Confusion matrix saved to: {cm_file}")
        
        # ê²°ê³¼ íŒŒì¼ ì¶œë ¥
        print(f"\nğŸ“ Results saved to: {self.output_dir}")
        print(f"  ğŸ“„ JSON: {results_file}")
        print(f"  ğŸ“Š Summary CSV: {summary_csv}")
        print(f"  ğŸ“ˆ Class CSV: {class_csv}")
        if 'clf_csv' in locals():
            print(f"  ğŸ¯ Classification CSV: {clf_csv}")
        
        return results
    
    def _print_detailed_results(self, detection_metrics, detailed_stats, classification_results):
        """ìƒì„¸ ê²°ê³¼ ì¶œë ¥"""
        print("\n" + "="*70)
        print("DETECTION í‰ê°€ ê²°ê³¼")
        print("="*70)
        
        stats = detailed_stats["total_statistics"]
        class_stats = detailed_stats["class_statistics"]
        
        print(f"\n[ì „ì²´ í†µê³„]")
        print(f"  ì´ Ground Truth ê°ì²´ ìˆ˜: {stats['total_ground_truths']}")
        print(f"  ì´ ì˜ˆì¸¡ëœ ê°ì²´ ìˆ˜ (threshold > {self.score_threshold}): {stats['total_predictions']}")
        
        print(f"\n[Detection ì„±ëŠ¥ (IoU >= {self.iou_threshold})]")
        if stats['total_predictions'] > 0:
            print(f"  True Positives (TP):  {stats['total_tp']:4d} ({stats['total_tp']/stats['total_predictions']*100:.1f}%)")
            print(f"  False Positives (FP): {stats['total_fp']:4d} ({stats['total_fp']/stats['total_predictions']*100:.1f}%)")
        else:
            print(f"  True Positives (TP):  {stats['total_tp']:4d}")
            print(f"  False Positives (FP): {stats['total_fp']:4d}")
        
        if stats['total_ground_truths'] > 0:
            print(f"  False Negatives (FN): {stats['total_fn']:4d} ({stats['total_fn']/stats['total_ground_truths']*100:.1f}%)")
        else:
            print(f"  False Negatives (FN): {stats['total_fn']:4d}")
        
        print(f"\n[ì „ì²´ Precision / Recall / F1]")
        print(f"  Precision: {stats['overall_precision']:.4f} ({stats['overall_precision']*100:.2f}%)")
        print(f"  Recall:    {stats['overall_recall']:.4f} ({stats['overall_recall']*100:.2f}%)")
        print(f"  F1-Score:  {stats['overall_f1']:.4f} ({stats['overall_f1']*100:.2f}%)")
        
        print(f"\n[mAP ì§€í‘œ]")
        print(f"  mAP (IoU=0.50:0.95): {detection_metrics['map']:.4f}")
        print(f"  mAP@0.50: {detection_metrics['map_50']:.4f}")
        print(f"  mAP@0.75: {detection_metrics['map_75']:.4f}")
        
        # í´ë˜ìŠ¤ë³„ ìƒì„¸ í†µê³„
        class_names = self.config['data']['class_names']
        print(f"\n[í´ë˜ìŠ¤ë³„ ìƒì„¸ í†µê³„]")
        for i, class_name in enumerate(class_names):
            if i < len(class_stats['class_tp']):
                print(f"\n  {class_name}:")
                print(f"    GT ê°ì²´ ìˆ˜: {class_stats['class_ground_truths'][i]}")
                print(f"    ì˜ˆì¸¡ ê°ì²´ ìˆ˜: {class_stats['class_predictions'][i]}")
                print(f"    TP: {class_stats['class_tp'][i]}, FP: {class_stats['class_fp'][i]}, FN: {class_stats['class_fn'][i]}")
                print(f"    Precision: {class_stats['class_precision'][i]:.4f}")
                print(f"    Recall: {class_stats['class_recall'][i]:.4f}")
                print(f"    F1-Score: {class_stats['class_f1'][i]:.4f}")
        
        # Per-class mAP
        if 'map_per_class' in detection_metrics:
            print(f"\n[í´ë˜ìŠ¤ë³„ mAP (IoU=0.50:0.95)]")
            map_per_class = detection_metrics['map_per_class']
            
            if map_per_class.ndim == 0:
                if len(class_names) > 0:
                    print(f"  {class_names[0]}: {map_per_class:.4f}")
            else:
                for i, ap in enumerate(map_per_class):
                    if i < len(class_names):
                        print(f"  {class_names[i]}: {ap:.4f}")
        
        # Classification ê²°ê³¼
        if classification_results:
            print("\n" + "="*70)
            print("CLASSIFICATION í‰ê°€ ê²°ê³¼")
            print("="*70)
            
            print(f"\n[ì „ì²´ ë¶„ë¥˜ ì„±ëŠ¥]")
            print(f"  Accuracy: {classification_results['accuracy']:.4f}")
            print(f"  Precision (macro): {classification_results['precision_macro']:.4f}")
            print(f"  Recall (macro): {classification_results['recall_macro']:.4f}")
            print(f"  F1-Score (macro): {classification_results['f1_macro']:.4f}")
            
            if 'classification_report' in classification_results:
                print(f"\n[í´ë˜ìŠ¤ë³„ ë¶„ë¥˜ ì„±ëŠ¥]")
                report = classification_results['classification_report']
                for class_name in class_names:
                    if class_name in report:
                        class_report = report[class_name]
                        print(f"  {class_name}:")
                        print(f"    Precision: {class_report['precision']:.4f}")
                        print(f"    Recall: {class_report['recall']:.4f}")
                        print(f"    F1-Score: {class_report['f1-score']:.4f}")
                        print(f"    Support: {class_report['support']}")
    
    def _save_inference_images(self, model, dataset, predictions, targets, max_images=None):
        """ì¸í¼ëŸ°ìŠ¤ ê²°ê³¼ ì´ë¯¸ì§€ ì €ì¥"""
        from PIL import Image, ImageDraw, ImageFont
        import torchvision.transforms as transforms
        
        # ì¸í¼ëŸ°ìŠ¤ ì´ë¯¸ì§€ ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
        inference_dir = self.output_dir / "inference_images"
        inference_dir.mkdir(exist_ok=True)
        
        device = next(model.parameters()).device
        model.eval()
        
        saved_count = 0
        class_names = self.config['data']['class_names']
        
        # max_images ì²˜ë¦¬: Noneì´ë©´ ì „ì²´ ë°ì´í„°ì…‹ í¬ê¸° ì‚¬ìš©
        if max_images is None:
            max_images = len(dataset)
        
        # ì‹¤ì œ ì²˜ë¦¬í•  ì´ë¯¸ì§€ ìˆ˜
        total_images = min(len(dataset), max_images)
        
        # ìƒ‰ìƒ ì •ì˜ (í´ë˜ìŠ¤ë³„)
        colors = {
            0: (255, 0, 0),    # fully-ripe: ë¹¨ê°•
            1: (255, 165, 0),  # semi-ripe: ì£¼í™©
            2: (0, 255, 0),    # unripe: ì´ˆë¡
        }
        
        print(f"Saving inference images to: {inference_dir}")
        print(f"Total images to process: {total_images}")
        
        with torch.no_grad():
            for idx in range(total_images):
                # ë°ì´í„°ì…‹ì—ì„œ ì´ë¯¸ì§€ì™€ íƒ€ê²Ÿ ê°€ì ¸ì˜¤ê¸°
                sample = dataset[idx]
                
                # ì´ë¯¸ì§€ ì²˜ë¦¬
                if isinstance(sample, dict):
                    image_tensor = sample['pixel_values']
                    target = sample['labels'] if 'labels' in sample else None
                else:
                    image_tensor, target = sample
                
                # ë°°ì¹˜ ì°¨ì› ì¶”ê°€
                if image_tensor.dim() == 3:
                    image_tensor = image_tensor.unsqueeze(0)
                
                # ëª¨ë¸ ì˜ˆì¸¡
                image_tensor = image_tensor.to(device)
                outputs = model(image_tensor)
                
                # ì˜ˆì¸¡ ê²°ê³¼ ì²˜ë¦¬
                probs = outputs.logits.softmax(-1)[0, :, :-1]
                scores, pred_labels = probs.max(-1)
                pred_boxes = outputs.pred_boxes[0]
                
                # ì„ê³„ê°’ ì ìš©
                keep = scores > self.score_threshold
                pred_boxes_filtered = pred_boxes[keep]
                pred_scores_filtered = scores[keep]
                pred_labels_filtered = pred_labels[keep]
                
                # ì´ë¯¸ì§€ë¥¼ PILë¡œ ë³€í™˜
                # ì •ê·œí™” í•´ì œ (ImageNet ê¸°ì¤€)
                mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)
                std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)
                
                image_denorm = image_tensor[0].cpu() * std + mean
                image_denorm = torch.clamp(image_denorm, 0, 1)
                
                # PIL ì´ë¯¸ì§€ë¡œ ë³€í™˜
                to_pil = transforms.ToPILImage()
                pil_image = to_pil(image_denorm)
                
                # ì´ë¯¸ì§€ í¬ê¸°
                img_w, img_h = pil_image.size
                
                # ê·¸ë¦¬ê¸° ì¤€ë¹„
                draw = ImageDraw.Draw(pil_image)
                
                try:
                    # í°íŠ¸ ì„¤ì • (ì‹œìŠ¤í…œì— ë”°ë¼ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ)
                    font = ImageFont.truetype("/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf", 16)
                except:
                    font = ImageFont.load_default()
                
                # Ground Truth ê·¸ë¦¬ê¸° (íŒŒë€ìƒ‰ í…Œë‘ë¦¬)
                if target is not None:
                    gt_boxes = target.get('boxes', [])
                    gt_labels = target.get('class_labels', [])
                    
                    if len(gt_boxes) > 0:
                        for box, label in zip(gt_boxes, gt_labels):
                            # CXCYWHë¥¼ XYXYë¡œ ë³€í™˜
                            cx, cy, w, h = box
                            x1 = (cx - w/2) * img_w
                            y1 = (cy - h/2) * img_h
                            x2 = (cx + w/2) * img_w
                            y2 = (cy + h/2) * img_h
                            
                            # ë°•ìŠ¤ ê·¸ë¦¬ê¸°
                            draw.rectangle([x1, y1, x2, y2], outline="blue", width=3)
                            
                            # ë¼ë²¨ í…ìŠ¤íŠ¸
                            if label.item() < len(class_names):
                                text = f"GT: {class_names[label.item()]}"
                                draw.text((x1, y1-20), text, fill="blue", font=font)
                
                # Predictions ê·¸ë¦¬ê¸° (í´ë˜ìŠ¤ë³„ ìƒ‰ìƒ)
                for box, label, score in zip(pred_boxes_filtered, pred_labels_filtered, pred_scores_filtered):
                    # CXCYWHë¥¼ XYXYë¡œ ë³€í™˜
                    cx, cy, w, h = box
                    x1 = (cx - w/2) * img_w
                    y1 = (cy - h/2) * img_h
                    x2 = (cx + w/2) * img_w
                    y2 = (cy + h/2) * img_h
                    
                    # í´ë˜ìŠ¤ë³„ ìƒ‰ìƒ
                    color = colors.get(label.item(), (255, 255, 255))
                    
                    # ë°•ìŠ¤ ê·¸ë¦¬ê¸°
                    draw.rectangle([x1, y1, x2, y2], outline=color, width=2)
                    
                    # ë¼ë²¨ê³¼ ìŠ¤ì½”ì–´ í…ìŠ¤íŠ¸
                    if label.item() < len(class_names):
                        text = f"{class_names[label.item()]}: {score:.2f}"
                        
                        # í…ìŠ¤íŠ¸ ë°°ê²½
                        bbox = draw.textbbox((x1, y2+5), text, font=font)
                        draw.rectangle(bbox, fill=color)
                        draw.text((x1, y2+5), text, fill="white", font=font)
                
                # ì´ë¯¸ì§€ ì €ì¥
                filename = f"inference_{idx:04d}_{self.split}.jpg"
                save_path = inference_dir / filename
                pil_image.save(save_path, quality=95)
                
                saved_count += 1
                
                if saved_count % 10 == 0:
                    print(f"  Saved {saved_count}/{total_images} images...")
        
        print(f"âœ… Saved {saved_count} inference images to: {inference_dir}")
        
        # ë²”ë¡€ ì´ë¯¸ì§€ ìƒì„±
        self._create_legend_image(inference_dir, class_names, colors)

    def _create_legend_image(self, inference_dir, class_names, colors):
        """ë²”ë¡€ ì´ë¯¸ì§€ ìƒì„±"""
        from PIL import Image, ImageDraw, ImageFont
        
        # ë²”ë¡€ ì´ë¯¸ì§€ í¬ê¸°
        legend_width = 400
        legend_height = 200
        
        # ë²”ë¡€ ì´ë¯¸ì§€ ìƒì„±
        legend_img = Image.new('RGB', (legend_width, legend_height), 'white')
        draw = ImageDraw.Draw(legend_img)
        
        try:
            font = ImageFont.truetype("/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf", 16)
            title_font = ImageFont.truetype("/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf", 20)
        except:
            font = ImageFont.load_default()
            title_font = ImageFont.load_default()
        
        # ì œëª©
        draw.text((20, 20), "Detection Legend", fill="black", font=title_font)
        
        # Ground Truth
        draw.rectangle([20, 60, 40, 80], outline="blue", width=3)
        draw.text((50, 65), "Ground Truth", fill="blue", font=font)
        
        # ê° í´ë˜ìŠ¤
        y_offset = 90
        for i, class_name in enumerate(class_names):
            color = colors.get(i, (255, 255, 255))
            draw.rectangle([20, y_offset, 40, y_offset+20], outline=color, width=3)
            draw.text((50, y_offset+5), f"Predicted: {class_name}", fill=color, font=font)
            y_offset += 30
        
        # ë²”ë¡€ ì €ì¥
        legend_path = inference_dir / "legend.jpg"
        legend_img.save(legend_path, quality=95)
        print(f"ğŸ“‹ Legend saved to: {legend_path}")

    def run(self):
        """í‰ê°€ ì‹¤í–‰"""
        print("\n" + "="*70)
        print("EVALUATION STARTED")
        print("="*70)
        print(f"Model: {self.config['model']['arch_name']}")
        print(f"Dataset: {self.config['data']['dataset_name']}")
        print(f"Split: {self.split}")
        print(f"Checkpoint: {Path(self.checkpoint_path).name}")
        print("="*70)
        
        # 1. ëª¨ë¸ ë° ë°ì´í„° ë¡œë“œ
        print("\nLoading model and dataset...")
        model, dataset, collate_fn = self._load_model_and_data()
        print(f"Dataset samples: {len(dataset)}")
        
        # 2. DataLoader ìƒì„±
        dataloader = self._create_dataloader(dataset, collate_fn)
        print(f"Evaluation batches: {len(dataloader)}")
        
        # 3. ìƒì„¸ Detection ë©”íŠ¸ë¦­ ê³„ì‚°
        print("\nCalculating detailed detection metrics...")
        detection_results, detailed_stats, predictions, targets = self._evaluate_detection_metrics_detailed(model, dataloader)
        
        # 4. Classification ë©”íŠ¸ë¦­ ê³„ì‚°
        print("Calculating classification metrics...")
        classification_results = self._evaluate_classification_metrics(predictions, targets)
        
        # 5. ì¸í¼ëŸ°ìŠ¤ ì´ë¯¸ì§€ ì €ì¥ (ìƒˆë¡œ ì¶”ê°€)
        print("Saving inference images...")
        self._save_inference_images(model, dataset, predictions, targets, max_images=None)
        
        # 6. ê²°ê³¼ ì €ì¥ ë° ì‹œê°í™”
        print("\nSaving results and creating visualizations...")
        final_results = self._save_results_with_visualization(detection_results, detailed_stats, classification_results)
        
        # 7. ìƒì„¸ ê²°ê³¼ ì¶œë ¥
        self._print_detailed_results(detection_results, detailed_stats, classification_results)
        
        print("\n" + "="*70)
        print("EVALUATION COMPLETED!")
        print("="*70)
        print(f"Results directory: {self.output_dir}")
        print(f"  - evaluation_results.json: ì „ì²´ ê²°ê³¼")
        print(f"  - summary_metrics.csv: ì„±ëŠ¥ ìš”ì•½í‘œ")
        print(f"  - class_metrics.csv: í´ë˜ìŠ¤ë³„ ì„±ëŠ¥í‘œ")
        if classification_results:
            print(f"  - classification_metrics.csv: ë¶„ë¥˜ ì„±ëŠ¥í‘œ")
            print(f"  - confusion_matrix.png: í˜¼ë™ í–‰ë ¬")
        print(f"  - inference_images/: ì¸í¼ëŸ°ìŠ¤ ê²°ê³¼ ì´ë¯¸ì§€ë“¤")
        print("="*70 + "\n")
        
        return final_results